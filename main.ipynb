{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, Docx2txtLoader\n",
    "from pathlib import Path\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from langchain.memory import ConversationSummaryBufferMemory,ConversationBufferMemory, ConversationBufferWindowMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import create_retrieval_chain, RetrievalQA, ConversationalRetrievalChain, RetrievalQAWithSourcesChain\n",
    "\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_VECTOR_STORE_DIR = Path('./data')\n",
    "# get hugging face API key from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def langchain_document_loader(TMP_DIR):\n",
    "    \"\"\"\n",
    "    Load documents from the temporary directory (TMP_DIR). \n",
    "    Files can be in txt, pdf, CSV or docx format.\n",
    "    \"\"\"\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    # txt_loader = DirectoryLoader(\n",
    "    #     TMP_DIR.as_posix(), glob=\"**/*.txt\", loader_cls=TextLoader, show_progress=True\n",
    "    # )\n",
    "    # documents.extend(txt_loader.load())\n",
    "\n",
    "    # pdf_loader = DirectoryLoader(\n",
    "    #     TMP_DIR.as_posix(), glob=\"**/*.pdf\", loader_cls=PyPDFLoader, show_progress=True\n",
    "    # )\n",
    "    # documents.extend(pdf_loader.load())\n",
    "\n",
    "    # csv_loader = DirectoryLoader(\n",
    "    #     TMP_DIR.as_posix(), glob=\"**/*.csv\", loader_cls=CSVLoader, show_progress=True,\n",
    "    #     loader_kwargs={\"encoding\":\"utf8\"}\n",
    "    # )\n",
    "    # documents.extend(csv_loader.load())\n",
    "\n",
    "    doc_loader = DirectoryLoader(\n",
    "        TMP_DIR.as_posix(),\n",
    "        glob=\"**/*.docx\",\n",
    "        loader_cls=Docx2txtLoader,\n",
    "        show_progress=True,\n",
    "    )\n",
    "    documents.extend(doc_loader.load())\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 138.66it/s]\n"
     ]
    }
   ],
   "source": [
    "directory_path = 'course reviews'\n",
    "TMP_DIR = Path(directory_path)\n",
    "documents = langchain_document_loader(TMP_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Year of study: Senior\\n\\nReview for 100 level course:\\n\\nCS100 - Computational Problem Solving\\nThis is a very slow and repetitive course. It takes too long to get to the meat of the content and there is a huge lag in between the labs and the lecture content. The labs and assignments were laughably easy which meant the grading ended up being terrible, with ridiculously high means. Would only recommend if someone plans to pursue a CS minor.\\n\\nGpa: 3.60-4.00' metadata={'source': 'course reviews\\\\Student_10_Course_100.docx'}\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "print(documents[0])\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_embedding_model():\n",
    "    embedding = OllamaEmbeddings(model='nomic-embed-text')\n",
    "    return embedding\n",
    "\n",
    "embeddings_nomic = select_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorstore(embeddings,documents,vectorstore_name):\n",
    "    \"\"\"Create a Chroma vector database.\"\"\"\n",
    "    persist_directory = (LOCAL_VECTOR_STORE_DIR.as_posix() + \"/\" + vectorstore_name)\n",
    "    vector_store = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarty of sentences (0, 1): 331.425\n",
      "Similarty of sentences (0, 2): 120.974\n",
      "Similarty of sentences (1, 2): 170.213\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentences = [\"I like pets.\",\n",
    "             \"Pets bring joy to our lives.\",\n",
    "             \"Langchain is a framework for developing applications powered by LLMs.\"]\n",
    "\n",
    "# 1. Calculate embedding vectors\n",
    "embedding_vectors = [embeddings_nomic.embed_query(sentence) for sentence in sentences]\n",
    "\n",
    "for combination in list(combinations(range(len(sentences)),2)):\n",
    "    # 2. Calculate similarity using dot product from numpy:\n",
    "    dot_prodduct = round(np.dot(embedding_vectors[combination[0]], embedding_vectors[combination[1]]),3)\n",
    "    print(f\"Similarty of sentences {combination}: {dot_prodduct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import chromadb python package. Please install it with `pip install chromadb`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\musta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:81\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[1;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'chromadb'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m create_vectorstores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;66;03m# change to True to create vectorstores\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m create_vectorstores:\n\u001b[1;32m----> 4\u001b[0m     vector_store_nomic \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_vectorstore\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings_nomic\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvector_store_nomic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVector store created\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m, in \u001b[0;36mcreate_vectorstore\u001b[1;34m(embeddings, documents, vectorstore_name)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create a Chroma vector database.\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m persist_directory \u001b[38;5;241m=\u001b[39m (LOCAL_VECTOR_STORE_DIR\u001b[38;5;241m.\u001b[39mas_posix() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m vectorstore_name)\n\u001b[1;32m----> 4\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vector_store\n",
      "File \u001b[1;32mc:\\Users\\musta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:778\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[1;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    776\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    777\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\musta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:714\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[0;32m    683\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Type[Chroma],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    694\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Chroma:\n\u001b[0;32m    695\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a Chroma vectorstore from a raw documents.\u001b[39;00m\n\u001b[0;32m    696\u001b[0m \n\u001b[0;32m    697\u001b[0m \u001b[38;5;124;03m    If a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    712\u001b[0m \u001b[38;5;124;03m        Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 714\u001b[0m     chroma_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    723\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    724\u001b[0m         ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(uuid\u001b[38;5;241m.\u001b[39muuid4()) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m texts]\n",
      "File \u001b[1;32mc:\\Users\\musta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:84\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[1;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import chromadb python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install chromadb`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     87\u001b[0m     )\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_settings \u001b[38;5;241m=\u001b[39m client_settings\n",
      "\u001b[1;31mImportError\u001b[0m: Could not import chromadb python package. Please install it with `pip install chromadb`."
     ]
    }
   ],
   "source": [
    "create_vectorstores = True # change to True to create vectorstores\n",
    "\n",
    "if create_vectorstores:\n",
    "    vector_store_nomic = create_vectorstore(embeddings_nomic,documents,\"vector_store_nomic\")\n",
    "    print(\"Vector store created\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector_store_Ollama: 134 chunks.\n"
     ]
    }
   ],
   "source": [
    "vector_store_nomic = Chroma(persist_directory = LOCAL_VECTOR_STORE_DIR.as_posix() + \"/vector_store_nomic\", \n",
    "                            embedding_function=embeddings_nomic)\n",
    "print(\"vector_store_Ollama:\",vector_store_nomic._collection.count(),\"chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_documents(docs,search_with_score=False):\n",
    "    \"\"\"helper function to print documents.\"\"\"\n",
    "    if search_with_score:\n",
    "        # used for similarity_search_with_score\n",
    "        print(\n",
    "            f\"\\n{'-' * 100}\\n\".join(\n",
    "                [f\"Document {i+1}:\\n\\n\" + doc[0].page_content +\"\\n\\nscore:\"+str(round(doc[-1],3))+\"\\n\" \n",
    "                 for i, doc in enumerate(docs)]\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        # used for similarity_search or max_marginal_relevance_search\n",
    "        print(\n",
    "            f\"\\n{'-' * 100}\\n\".join(\n",
    "                [f\"Document {i+1}:\\n\\n\" + doc.page_content \n",
    "                 for i, doc in enumerate(docs)]\n",
    "            )\n",
    "        )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "Year of study: Junior\n",
      "\n",
      "Review for 200 level course:\n",
      "\n",
      "CS 202\n",
      "Data Structures with Sir Ihsan was a challenging yet fun course. I learned a lot and the course challenges your ability to think and rationalize. Learning wise, this course is great. The outline is well defined and you already know the quiz schedule before the semester so that helps you set your schedule before hand. There is no midterm either which helps during the midweek by lessening the burden. The assignments are comparatively easier but still challenging enough. \n",
      "\n",
      "Gpa: 3.60-4.00\n",
      "\n",
      "score:375.421\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "Year of study: Junior\n",
      "\n",
      "Review for 200 level course:\n",
      "\n",
      "CS 202\n",
      "Data Structures with Sir Ihsan was a challenging yet fun course. I learned a lot and the course challenges your ability to think and rationalize. Learning wise, this course is great. The outline is well defined and you already know the quiz schedule before the semester so that helps you set your schedule before hand. There is no midterm either which helps during the midweek by lessening the burden. The assignments are comparatively easier but still challenging enough. \n",
      "\n",
      "Gpa: 3.60-4.00\n",
      "\n",
      "score:375.421\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "Year of study: Senior\n",
      "\n",
      "Review for 100 level course:\n",
      "\n",
      "CS100 - Computational Problem Solving\n",
      "This is a very slow and repetitive course. It takes too long to get to the meat of the content and there is a huge lag in between the labs and the lecture content. The labs and assignments were laughably easy which meant the grading ended up being terrible, with ridiculously high means. Would only recommend if someone plans to pursue a CS minor.\n",
      "\n",
      "Gpa: 3.60-4.00\n",
      "\n",
      "score:401.399\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "Year of study: Senior\n",
      "\n",
      "Review for 100 level course:\n",
      "\n",
      "CS100 - Computational Problem Solving\n",
      "This is a very slow and repetitive course. It takes too long to get to the meat of the content and there is a huge lag in between the labs and the lecture content. The labs and assignments were laughably easy which meant the grading ended up being terrible, with ridiculously high means. Would only recommend if someone plans to pursue a CS minor.\n",
      "\n",
      "Gpa: 3.60-4.00\n",
      "\n",
      "score:401.399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get most similar documents - with scores \n",
    "# Here, we use Cosine Similarity. So a lower score is better.\n",
    "\n",
    "query = 'What are some difficult CS courses?'\n",
    "docs_withScores = vector_store_nomic.similarity_search_with_score(query,k=4)\n",
    "\n",
    "print_documents(docs_withScores,search_with_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarty of document_0 to the query: 190.1073\n",
      "Similarty of document_1 to the query: 190.1073\n",
      "Similarty of document_2 to the query: 212.1687\n",
      "Similarty of document_3 to the query: 212.1687\n"
     ]
    }
   ],
   "source": [
    "query_embeddings = embeddings_nomic.embed_query(query)\n",
    "docs_embeddings = embeddings_nomic.embed_documents(\n",
    "    [docs_withScores[i][0].page_content \n",
    "     for i in range(len(docs_withScores))\n",
    "    ]\n",
    ")\n",
    "\n",
    "for i in range(len(docs_embeddings)):\n",
    "    dot_product = round(np.dot(query_embeddings, docs_embeddings[i]),4)\n",
    "    print(f\"Similarty of document_{i} to the query: {dot_product}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vectorstore_backed_retriever(vectorstore,search_type=\"similarity\",k=4,score_threshold=None):\n",
    "    \"\"\"create a vectorsore-backed retriever\n",
    "    Parameters: \n",
    "        search_type: Defines the type of search that the Retriever should perform.\n",
    "            Can be \"similarity\" (default), \"mmr\", or \"similarity_score_threshold\"\n",
    "        k: number of documents to return (Default: 4) \n",
    "        score_threshold: Minimum relevance threshold for similarity_score_threshold (default=None)\n",
    "    \"\"\"\n",
    "    search_kwargs={}\n",
    "    if k is not None:\n",
    "        search_kwargs['k'] = k\n",
    "    if score_threshold is not None:\n",
    "        search_kwargs['score_threshold'] = score_threshold\n",
    "\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=search_type,\n",
    "        search_kwargs=search_kwargs\n",
    "    )\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "Year of study: Junior\n",
      "\n",
      "Review for 200 level course:\n",
      "\n",
      "CS 202\n",
      "Data Structures with Sir Ihsan was a challenging yet fun course. I learned a lot and the course challenges your ability to think and rationalize. Learning wise, this course is great. The outline is well defined and you already know the quiz schedule before the semester so that helps you set your schedule before hand. There is no midterm either which helps during the midweek by lessening the burden. The assignments are comparatively easier but still challenging enough. \n",
      "\n",
      "Gpa: 3.60-4.00\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "Year of study: Junior\n",
      "\n",
      "Review for 200 level course:\n",
      "\n",
      "CS 202\n",
      "Data Structures with Sir Ihsan was a challenging yet fun course. I learned a lot and the course challenges your ability to think and rationalize. Learning wise, this course is great. The outline is well defined and you already know the quiz schedule before the semester so that helps you set your schedule before hand. There is no midterm either which helps during the midweek by lessening the burden. The assignments are comparatively easier but still challenging enough. \n",
      "\n",
      "Gpa: 3.60-4.00\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "Year of study: Junior\n",
      "\n",
      "Review for 400 level course:\n",
      "\n",
      "CA 437 - Deep Learrning. Intellectually challenging and stimulating. Dl introduces the many used practical applications of different algorithms. The course, up until now, has thoroughly tested our understanding of different concepts and its application in coding. Very good and practical course\n",
      "\n",
      "Gpa: 3.30-3.60\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "Year of study: Junior\n",
      "\n",
      "Review for 400 level course:\n",
      "\n",
      "CA 437 - Deep Learrning. Intellectually challenging and stimulating. Dl introduces the many used practical applications of different algorithms. The course, up until now, has thoroughly tested our understanding of different concepts and its application in coding. Very good and practical course\n",
      "\n",
      "Gpa: 3.30-3.60\n"
     ]
    }
   ],
   "source": [
    "# Similarity search\n",
    "retriever = Vectorstore_backed_retriever(vector_store_nomic,search_type=\"similarity\",k=4)\n",
    "\n",
    "# Get relevant documents\n",
    "\n",
    "query = 'What are some intellectually challenging and stimulating courses?'\n",
    "relevant_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "print_documents(relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiating LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 27\u001b[0m\n\u001b[0;32m     13\u001b[0m     llm \u001b[38;5;241m=\u001b[39m HuggingFaceHub(\n\u001b[0;32m     14\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-Instruct-v0.2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     15\u001b[0m         \u001b[38;5;66;03m# repo_id=\"meta-llama/Meta-Llama-3-70B-Instruct\", \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m         },\n\u001b[0;32m     24\u001b[0m     )\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m llm\n\u001b[1;32m---> 27\u001b[0m HUGGINGFACE_API_KEY \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHUGGINGFACE_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m llm \u001b[38;5;241m=\u001b[39m instantiate_LLM(api_key\u001b[38;5;241m=\u001b[39mHUGGINGFACE_API_KEY)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "def instantiate_LLM(api_key,temperature=0.5,top_p=0.95,model_name=None):\n",
    "    \"\"\"Instantiate LLM in Langchain.\n",
    "    Parameters:\n",
    "        LLM_provider (str): the LLM provider; in [\"OpenAI\",\"Google\",\"HuggingFace\"]\n",
    "        model_name (str): in [\"gpt-3.5-turbo\", \"gpt-3.5-turbo-0125\", \"gpt-4-turbo-preview\", \n",
    "            \"gemini-pro\", \"mistralai/Mistral-7B-Instruct-v0.2\"].            \n",
    "        api_key (str): google_api_key or openai_api_key or huggingfacehub_api_token \n",
    "        temperature (float): Range: 0.0 - 1.0; default = 0.5\n",
    "        top_p (float): : Range: 0.0 - 1.0; default = 1.\n",
    "    \"\"\"\n",
    "    \n",
    "  \n",
    "    llm = HuggingFaceHub(\n",
    "        repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\", \n",
    "        # repo_id=\"meta-llama/Meta-Llama-3-70B-Instruct\", \n",
    "        # repo_id=model_name,\n",
    "        huggingfacehub_api_token=api_key,\n",
    "        model_kwargs={\n",
    "            \"temperature\":temperature,\n",
    "            \"top_p\": top_p,\n",
    "            \"do_sample\": True,\n",
    "            \"max_new_tokens\":1024\n",
    "        },\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "llm = instantiate_LLM(api_key=HUGGINGFACE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_memory():\n",
    "    \"\"\"Creates a ConversationSummaryBufferMemory for our model\n",
    "    Creates a ConversationBufferWindowMemory for our models.\"\"\"\n",
    "    \n",
    "    memory = ConversationBufferWindowMemory(\n",
    "        memory_key=\"history\",\n",
    "        input_key=\"question\",\n",
    "        return_messages=True,\n",
    "        k=3\n",
    "    )\n",
    "\n",
    "    return memory\n",
    "\n",
    "memory = create_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(\n",
    "    {\"question\": \"What can you do?\"},\n",
    "    {\"output\": \"I can answer queries based on the past reviews and course outlines of various courses offered at LUMS.\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_qa = \"\"\"\n",
    "You are a professional chatbot assistant for helping students at LUMS regarding course selection.\n",
    "\n",
    "Please follow the following rules:\n",
    "\n",
    "1. Answer the question in your own words from the context given to you.\n",
    "2. If you don't know the answer, don't try to make up an answer.\n",
    "3. If you don't have a course's review or outline, just say that you do not know about this course.\n",
    "4. If a user enters a course code (e.g. ECON100 or CS370), match it with reviews with that course code. If the user enters a course name (e.g. Introduction to Economics or Database Systems), match it with reviews with that course name.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "You are having a converation with a student at LUMS.\n",
    "\n",
    "Chat History: {history}\n",
    "\n",
    "Human: {question}\n",
    "\n",
    "Assistant123:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"context\", \"question\"],\n",
    "    template=context_qa\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-pro\n",
      "models/gemini-pro-vision\n"
     ]
    }
   ],
   "source": [
    "for m in genai.list_models():\n",
    "  if 'generateContent' in m.supported_generation_methods:\n",
    "    print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    verbose=False,\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": prompt,\n",
    "        \"memory\": memory\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  .  .  .   \n",
      "Me: Should i take the course with code REL500\n",
      "Chatbot: \n",
      "I'm sorry for any confusion, but I don't have a review or outline for the course with code REL500. Could you please provide more details about the course, such as its name or a brief description, so I can try to help you with any related queries?\n",
      "  .  .  .   \n",
      "Me: should i take REL523\n",
      "Chatbot: \n",
      "I'm unable to provide information about the course REL523 as I don't have any reviews or outlines related to it in my database. Could you please check the course name or code with your academic advisor or the department offering the course for more accurate and detailed information?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/93/p8_82pyd6nxg7_p_xtp6xv100000gn/T/ipykernel_57259/2978624241.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your question here (or type 'exit' to quit): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Exiting ChatBot. Goodbye!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"Enter your question here (or type 'exit' to quit): \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        print(\"Exiting ChatBot. Goodbye!\")\n",
    "        break\n",
    "    print(\"  .  .  .   \")\n",
    "    #result = agent(user_input)\n",
    "\n",
    "    result = qa({'query': user_input})\n",
    "\n",
    "    # print(\"result:\", result)\n",
    "\n",
    "    answer = result['result']\n",
    "\n",
    "    # only keep the part followed by 'Assistant123:'\n",
    "    answer = answer.split('Assistant123:')[-1]\n",
    "    print(\"Me:\", user_input)\n",
    "    print(\"Chatbot:\", answer)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
